{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B7KhVUtzaJNO",
        "73X6qkEznL_a"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UofT FASE ML Bootcamp\n",
        "#### Friday June 14, 2024\n",
        "####  Word Embeddings - Properties, Meaning and Training - Lab 1, Day 5\n",
        "#### Teaching team: Eldan Cohen, Alex Olson, Nakul Upadhya, Hriday Chheda\n",
        "##### Based on CARTE-DSI ML Bootcamp 2023 notebook by Prof. Jonathan Rose\n",
        "\n",
        "This lab engages you in the properties, meaning, viewing and training of word embeddings (also called word vectors). The specific learning objectives in this assignment are:\n",
        "\n",
        "1.   To learn word embedding properties, and use them in simple ways.\n",
        "2.   (optional) To translate vectors into understandable categories of meaning\n",
        "3.   To understand how embeddings are created, using the Skip Gram method.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qDdF-DpXWg1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Experimenting and Understanding Word Embedding/Vectors\n",
        "# Using the GloVe Embeddings\n",
        "\n",
        "\n",
        "Word embeddings (also known as word vectors) are a way to encode the meaning of words into a set of numbers.\n",
        "\n",
        "These embeddings are created by training a neural network model using many examples of the use of language.  These examples could be the whole of Wikipedia or a large collection of news articles.\n",
        "\n",
        "To start, we will explore a set of word embeddings that someone else took the time and computational power to create. One of the most commonly-used pre-trained word embeddings are the **GloVe embeddings**."
      ],
      "metadata": {
        "id": "UMKnFM6Wkl46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Embeddings\n",
        "\n",
        "You can read about the GloVe embeddings here: https://nlp.stanford.edu/projects/glove/, and read the original paper describing how they work here: https://nlp.stanford.edu/pubs/glove.pdf.\n",
        "\n",
        "There are several variations of GloVe embeddings. They differ in the text used to train the embedding, and the *size* of the embeddings.\n",
        "\n",
        "Throughout this lab we'll use a package called `torchtext`, that is part of PyTorch.\n",
        "\n",
        "We'll begin by loading a set of GloVe embeddings. The first time you run the code below, it will cause the download of a large file (862MB) containing the embeddings."
      ],
      "metadata": {
        "id": "rNg2XmmFlfuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "import torch\n",
        "import torchtext\n",
        "import pandas as pd\n",
        "torchtext.disable_torchtext_deprecation_warning()\n",
        "from torchtext.vocab import GloVe"
      ],
      "metadata": {
        "id": "PjRMuu2Fnb9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypuiFEIYWfkz"
      },
      "outputs": [],
      "source": [
        "# The first time you run this will download a ~862MB file\n",
        "glove = GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
        "              dim=50)  # embedding size = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the loaded glove embeddings to look up the embeddings of individual words.\n",
        "For example, let's look at what the embedding of the word \"apple\" looks like:"
      ],
      "metadata": {
        "id": "NiKD_dpmtuWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove['apple']"
      ],
      "metadata": {
        "id": "M4i4ocTQmIYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the output above, the embedding of a given word is a torch tensor with dimension `(50,)`. We don't know what the meaning of each number is, but we do know that there are properties of the embeddings that can be observed. For example, `distances between embeddings` are meaningful.\n",
        "\n",
        "## Measuring Distance\n",
        "\n",
        "Let's consider one specific metric of distance between two embedding vectors called the **Euclidean distance**. The Euclidean distance of two vectors $x = [x_1, x_2, ... x_n]$ and\n",
        "$y = [y_1, y_2, ... y_n]$ is just the 2-norm of their difference $x - y$. We can compute\n",
        "the Euclidean distance between $x$ and $y$: $\\sqrt{\\sum_i (x_i - y_i)^2}$\n",
        "\n",
        "The PyTorch function `torch.norm` computes the 2-norm of a vector for us, so we\n",
        "can compute the Euclidean distance between two vectors like this:"
      ],
      "metadata": {
        "id": "GB-dyAZjvdKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = glove['cat']\n",
        "y = glove['dog']\n",
        "torch.norm(y - x)"
      ],
      "metadata": {
        "id": "4hvhg6S-vNjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = glove['apple']\n",
        "b = glove['orange']\n",
        "torch.norm(b - a)"
      ],
      "metadata": {
        "id": "_897mUSTwAwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['bad'])"
      ],
      "metadata": {
        "id": "6Z-zVh8CwMtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['water'])"
      ],
      "metadata": {
        "id": "kgfIIcTxwO99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['well'])"
      ],
      "metadata": {
        "id": "dZAQ64sNwRfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['perfect'])"
      ],
      "metadata": {
        "id": "mEdfGmh7wmN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity\n",
        "\n",
        "An alternative and more commonly-used measure of distance is the **Cosine Similarity**. The cosine similarity measures the *angle* between two vectors, and has the property that it only considers the *direction* of the vectors, not their the magnitudes. It is computed as follows for two vectors A and B:\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1hSaQRBjH828lx1xozJCA4F0ZhiX2S0Xt)"
      ],
      "metadata": {
        "id": "FwCYQe_XxLik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#consider two vectors x and y\n",
        "#unsqueeze is used because cosine similarity wants at least 2-D inputs\n",
        "x = torch.tensor([1., 1., 1.]).unsqueeze(0)\n",
        "y = torch.tensor([2., 2., 2.]).unsqueeze(0)\n",
        "\n",
        "# Calculate the cosine similarity between x and y\n",
        "# Expect the cosine similarity to be 1.0 since x and y are in the same direction\n",
        "torch.cosine_similarity(x, y)"
      ],
      "metadata": {
        "id": "cd8iVpD4wp28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cosine similarity is actually a *similarity* measure rather than a *distance* measure, and gives a result between -1 and 1. Thus, the larger the similarity, (closer to 1) the \"closer in meaning\" the word embeddings are to each other."
      ],
      "metadata": {
        "id": "-kAON3H92whE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.tensor([-1., -1., -1.]).unsqueeze(0)\n",
        "\n",
        "# Calculate the cosine similarity between x and z\n",
        "# Expect the cosine similarity to be -1.0 since x and z point in the opposite \"direction\"\n",
        "torch.cosine_similarity(x, z)"
      ],
      "metadata": {
        "id": "QNotYDzszo4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = glove['cat']\n",
        "y = glove['dog']\n",
        "torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))"
      ],
      "metadata": {
        "id": "CT_gUzm34S2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = glove['apple']\n",
        "b = glove['banana']\n",
        "torch.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))"
      ],
      "metadata": {
        "id": "2BsgHFVF4W9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['bad'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "pF8vDFMg4adK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['water'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "WCTpK0304mXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['well'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "on933r1J4emF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['perfect'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "LIk0UjYx4kwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['watermelon'].unsqueeze(0),\n",
        "                        glove['aeroplane'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "NrAwetzf4v77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: torch.cosine_similarity requires two dimensions to work, which is created with the unsqueeze option, illustrated in more detail below"
      ],
      "metadata": {
        "id": "GoYuQxTa5TwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = glove['good']\n",
        "print(x.shape) # [50]\n",
        "y = x.unsqueeze(0) # [1, 50]\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "d4OFVPiU5TRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Similarity\n",
        "\n",
        "Now that we have notions of distance and similarity in our embedding space, we can talk about words that are \"close\" to each other in the embedding space. For now, let's use Euclidean distances to look at how close various words are to the word \"cat\"."
      ],
      "metadata": {
        "id": "YR-6jQto5b8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'cat'\n",
        "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
        "for w in other:\n",
        "    dist = torch.norm(glove[word] - glove[w]) # euclidean distance\n",
        "    print(w, \"\\t%5.2f\" % float(dist))"
      ],
      "metadata": {
        "id": "5wg_H1Bk48ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the same thing with cosine similarity:"
      ],
      "metadata": {
        "id": "NT2_dyt-6LZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'cat'\n",
        "other = ['pet', 'dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
        "for w in other:\n",
        "    dist = torch.cosine_similarity(glove[word].unsqueeze(0),glove[w].unsqueeze(0)) # cosine distance\n",
        "    print(w, \"\\t%5.2f\" % float(dist))"
      ],
      "metadata": {
        "id": "jwBTNAz96AFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look through the entire **vocabulary** for words that are closest to a point in the embedding space -- for example, we can look for words that are closest to another word such as \"cat\"."
      ],
      "metadata": {
        "id": "QumucClS6b8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_closest_words(vec, n=5):\n",
        "    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n",
        "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
        "    closest_words = []\n",
        "    for idx, difference in lst[1:n+1]:                         # take the top n\n",
        "        print(glove.itos[idx], \"\\t%5.2f\" % difference)\n",
        "        closest_words.append(glove.itos[idx])\n",
        "    return closest_words\n",
        "\n",
        "closest_words_to_cat = print_closest_words(glove[\"cat\"], n=10)"
      ],
      "metadata": {
        "id": "6MxGErGA6O9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest_words_to_dog = print_closest_words(glove['dog'])"
      ],
      "metadata": {
        "id": "K6JQA84I6j4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest_words_to_nurse = print_closest_words(glove['nurse'])"
      ],
      "metadata": {
        "id": "mmQicawa8IL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest_words_to_computer = print_closest_words(glove['computer'])"
      ],
      "metadata": {
        "id": "AD3G-pMF8WjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can also try printing closest words to any other words of your choice here:\n",
        "\n"
      ],
      "metadata": {
        "id": "ThHIes6D8dJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could also look at which words are closest to the midpoints of two words:"
      ],
      "metadata": {
        "id": "yuDTCwCI90qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "closest_to_mid_1 = print_closest_words((glove['happy'] + glove['sad']) / 2)"
      ],
      "metadata": {
        "id": "PGusXTUB91R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest_to_mid_2 = print_closest_words((glove['lake'] + glove['building']) / 2)"
      ],
      "metadata": {
        "id": "BV-ISeLA96js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "closest_to_mid_3 = print_closest_words((glove['bravo'] + glove['michael']) / 2)"
      ],
      "metadata": {
        "id": "4wo_X6Bp-cC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1\n",
        "1.1.1 Write a new function, similar to print_closest_words called print_closest_cosine_words that prints out the N-most (where N is an input parameter) similar words using cosine similarity rather than euclidean distance.\n",
        "\n",
        "The documentation for the [sorted](https://python-reference.readthedocs.io/en/latest/docs/functions/sorted.html) method in python might help\n",
        "\n"
      ],
      "metadata": {
        "id": "qerwkoJVAL7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_closest_cosine_words(vec, n=5):\n",
        "  # TODO\n",
        "  sims =   # compute similarities to all words\n",
        "  lst =  # sort by similarity (descending order, remember higher similarity score, closer the word)\n",
        "  closest_words = []\n",
        "  # take the top n\n",
        "  # TODO\n",
        "  return closest_words"
      ],
      "metadata": {
        "id": "Q2KJmHdgAJkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.2 Create a table that compares the 10-most cosine-similar words to the word 'dog', in order, alongside to the 10 closest"
      ],
      "metadata": {
        "id": "BVnUuCl1BL_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "closest_euclidean_words = print_closest_words(glove['dog'])\n",
        "print(\"\\n\")\n",
        "closest_cosine_words = print_closest_cosine_words(glove['dog'])\n",
        "print(\"\\n\")\n",
        "table = pd.DataFrame()\n",
        "table[\"Euclidean\"] = closest_euclidean_words\n",
        "table[\"Cosine\"] = closest_cosine_words\n",
        "print(table)"
      ],
      "metadata": {
        "id": "2h4QDcObCtrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the same table for word \"computer\"\n",
        "# TODO"
      ],
      "metadata": {
        "id": "ihkV86ACKU7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1.3 Looking at the two lists, does one of the metrics (cosine similarity or euclidean distance) seem to be better than the other?\n",
        "\n",
        "TODO\n"
      ],
      "metadata": {
        "id": "LlD-Aty8KhwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Analogies\n",
        "\n",
        "One surprising aspect of word embeddings is that the *directions* in the embedding space can be meaningful. For example, some analogy-like relationships like this tend to hold:\n",
        "\n",
        "$$ king - man + woman \\approx queen $$\n",
        "\n",
        "Analogies show us how relationships between pairs of words that is captured in the learned vectors"
      ],
      "metadata": {
        "id": "F3D4X_Q0_uv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_closest_words(glove['king'] - glove['man'] + glove['woman'])"
      ],
      "metadata": {
        "id": "rq_MVbTd-cwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top result is a reasonable answer like \"queen\",  and the name of the queen of england.\n",
        "\n",
        "We can flip the analogy around and it works:"
      ],
      "metadata": {
        "id": "SObT9dicM8R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = print_closest_words(glove['queen'] - glove['woman'] + glove['man'])"
      ],
      "metadata": {
        "id": "HCOLWngBM-7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = print_closest_words(glove['king'] - glove['prince'] + glove['princess'])"
      ],
      "metadata": {
        "id": "5PbjuVO8M-0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = print_closest_words(glove['uncle'] - glove['man'] + glove['woman'])"
      ],
      "metadata": {
        "id": "fxG-FQe5M-xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = print_closest_words(glove['grandmother'] - glove['mother'] + glove['father'])"
      ],
      "metadata": {
        "id": "zV2imC-lM-ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = print_closest_words(glove['old'] - glove['young'] + glove['father'])"
      ],
      "metadata": {
        "id": "q3SH9xXJLcD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also move an embedding towards the direction of \"goodness\" or \"badness\":"
      ],
      "metadata": {
        "id": "5Y3xt9lmNle3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = print_closest_words(glove['good'] - glove['bad'] + glove['programmer'])"
      ],
      "metadata": {
        "id": "bZMiB3USNI_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = print_closest_words(glove['bad'] - glove['good'] + glove['programmer'])"
      ],
      "metadata": {
        "id": "vP_BSsCBNI8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2.1 Consider now the word pair relationships given in Figure 1 below, which comes from Table 1 of the Mikolov [[link](https://arxiv.org/abs/1301.3781)] paper. Choose one of these relationships, but not one of the ones already shown above, and report which one you chose. Write and run code that will generate the second word given the first word. Generate 10 more examples of the same relationship from 10 other words, and comment on the quality of the results.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1O7Zizu63jj5aoZkGkK0sz93CZSEsBDuW)\n",
        "\n"
      ],
      "metadata": {
        "id": "CJjQLdz_OAuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Choose one of the relationships from the table above and generate 10 examples\n",
        "\n"
      ],
      "metadata": {
        "id": "XgvVPnEvNI5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Change Embedding Dimension\n",
        "Now we change the embedding dimension (also known as the vector size) from 50 to 300 and re-run all the examples from above including the new cosini similarity function. Answer the following questions:\n",
        "1.   How does the euclidean distance change between the various words when switching from d=50 to d=300?\n",
        "2.   How does the cosine similarity change?\n",
        "3.   Does the ordering of nearness change?\n",
        "4.   Is it clear that the larger size vectors give better results - why or why not?\n",
        "\n"
      ],
      "metadata": {
        "id": "BhGfRBpRTe05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The first time you run this will download a ~862MB file\n",
        "glove = GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
        "              dim=300)  # embedding size = 300"
      ],
      "metadata": {
        "id": "3sYqu2QsUcFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the euclidean distances for embedding dimension of 300"
      ],
      "metadata": {
        "id": "un7HEyyWXgJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = glove['cat']\n",
        "y = glove['dog']\n",
        "torch.norm(y - x)"
      ],
      "metadata": {
        "id": "tUI6RtqFV3mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = glove['apple']\n",
        "b = glove['orange']\n",
        "torch.norm(b - a)"
      ],
      "metadata": {
        "id": "GGorX_x-XKf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['bad'])"
      ],
      "metadata": {
        "id": "8WUnfsbQXMqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['water'])"
      ],
      "metadata": {
        "id": "9WZZVevuXPDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['well'])"
      ],
      "metadata": {
        "id": "-KTyY1QdXQna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(glove['good'] - glove['perfect'])"
      ],
      "metadata": {
        "id": "WjbzYjY2XTiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.1 Compare to the euclidean distances from above (embedding dimension of 50) and answer question 1. How does the euclidean distance change between the various words when switching from d=50 to d=300?\n",
        "\n",
        "Answer: TODO"
      ],
      "metadata": {
        "id": "O5mqAc4oXsPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, lets look at cosine similarity with embedding dimension 300"
      ],
      "metadata": {
        "id": "sF6JaGB5ZzZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = glove['cat']\n",
        "y = glove['dog']\n",
        "torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))"
      ],
      "metadata": {
        "id": "4YQNQ7uhXVh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = glove['apple']\n",
        "b = glove['banana']\n",
        "torch.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))"
      ],
      "metadata": {
        "id": "-2Du66czYvZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['bad'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "lq8JNUnXYvXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['water'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "cyx_hox3ZMJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['well'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "TskeONmiYvT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['good'].unsqueeze(0),\n",
        "                        glove['perfect'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "xdymOV33Y0g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cosine_similarity(glove['watermelon'].unsqueeze(0),\n",
        "                        glove['aeroplane'].unsqueeze(0))"
      ],
      "metadata": {
        "id": "Y4zmxiCsZPKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.2 Compare to the cosine similarities from above (embedding dimension of 50) and answer question 2. How does the cosine similarity change when switching from d=50 to d=300?\n",
        "\n",
        "Answer: TODO"
      ],
      "metadata": {
        "id": "0BXhKAJKZXGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will look at the nearness of words"
      ],
      "metadata": {
        "id": "yG6phX-gZw9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "closest_euclidean_words = print_closest_words(glove['dog'])\n",
        "closest_cosine_words = print_closest_cosine_words(glove['dog'])\n",
        "table = pd.DataFrame()\n",
        "table[\"Euclidean\"] = closest_euclidean_words\n",
        "table[\"Cosine\"] = closest_cosine_words\n",
        "print(table)"
      ],
      "metadata": {
        "id": "UvmbvMpsZWx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.3 Compare to the near words from above (embedding dimension of 50) and answer question 3. Does the ordering of nearness words change?\n",
        "\n",
        "Answer: TODO"
      ],
      "metadata": {
        "id": "XPaBRJ5laDoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.4 Is it clear that the larger size vectors give better results - why or why not?\n",
        "\n",
        "Answer: TODO"
      ],
      "metadata": {
        "id": "LvGCAct7aWhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. (Optional): Computing Meaning from Word Embeddings\n",
        "\n",
        "Note: Attempt this section after you have finished the rest of the lab!\n",
        "\n",
        "Now that we’ve seen some of the power of word embeddings, we can also feel the frustration that the individual elements/numbers in each word vector do not have a meaning that can be interpreted or understood by humans. It would have preferable that each position in the vector correspond to a specific axis of meaning that we can understand based on our ability to comprehend language.\n",
        "\n",
        "For example the \"amount\" the word related to *colour* or *temperature* or *politics*. This is not the case, because the numbers are the result of an optmization process that does not drive each vector element toward human-understandable meaning.\n",
        "\n",
        "We can, however, make use of the methods shown in Section 1 above to measure the amount of meaning in specific categories of our choosing, such as colour. Suppose that we want to know how much a particular word/embedding relates to colour. One way to measure that could be to determine the cosine similarity between the word embedding for colour and the word of interest. We might expect that a word like ‘sky’ or ‘grass’ might have elements of colour in it, and that ‘purple’ would have more. However, it may also be true that there are multiple meanings to a single word, such as ‘colour’, and so it might be better to define a category of meaning by using several words that, all together, define it with more precision.\n",
        "\n",
        "For example, a way to define a category such as colour would be to use that word itself, and to- gether with several examples, such ‘red’, ‘green’, ‘blue’, ‘yellow.’ Then, to measure the “amount” of colour in a specific word (like ‘sky’) you could compute the average cosine similarity between sky and each of the words in the category. Alternatively, you could average the vectors of all the words in the category, and compute the cosine similarity between the embedding of sky and that average embedding. In this section, use the d=50 GlOVe embeddings that you used in Section 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "B7KhVUtzaJNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings using torchtext\n",
        "glove = GloVe(name='6B', dim=50)\n",
        "embedding_size = 50  # Size of GloVe embeddings"
      ],
      "metadata": {
        "id": "2VyMjlrdunHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1\n",
        "Write a PyTorch-based function called compare words to category that takes as input:\n",
        "* The meaning category given by a set of words (as discussed above) that describe the category, and\n",
        "* A given word to ‘measure’ against that category.\n",
        "\n",
        "\n",
        "The function should compute the cosine similarity of the given word in the category in two ways: \\\\\n",
        "(a) By averaging the cosine similarity of the given word with every word in the category, and \\\\\n",
        "(b) By computing the cosine similarity of the word with the average embedding of all of the words in the category."
      ],
      "metadata": {
        "id": "pEzXXUcbrmyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a word to its embedding\n",
        "def word_to_embedding(word, glove):\n",
        "    if word in glove.stoi:\n",
        "        return glove[word]\n",
        "    else:\n",
        "        return torch.zeros(embedding_size)\n",
        "\n",
        "def compare_words_to_category(word, category_words, glove):\n",
        "    word_embedding = word_to_embedding(word, glove).unsqueeze(0)\n",
        "    category_embeddings = torch.stack([word_to_embedding(w, glove) for w in category_words])\n",
        "\n",
        "    # Method (a): Average cosine similarity of the given word with every word in the category\n",
        "    cosine_similarities = # TODO\n",
        "    avg_cosine_similarity = cosine_similarities.mean().item()\n",
        "\n",
        "    # Method (b): Cosine similarity of the word with the average embedding of the category words\n",
        "    avg_category_embedding = # TODO\n",
        "    avg_category_cosine_similarity = torch.cosine_similarity(word_embedding, avg_category_embedding).item()\n",
        "\n",
        "    return avg_cosine_similarity, avg_category_cosine_similarity"
      ],
      "metadata": {
        "id": "_9nVGRP5rBGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2\n",
        "Let’s define the colour meaning category using these words: “colour”, “red”, “green”, “blue”, “yellow.” Compute the similarity (using both methods (a) and (b) above) for each of these words: “greenhouse”, “sky”, “grass”, “purple”, “scissors”, “microphone”, “president” and present them in a table."
      ],
      "metadata": {
        "id": "mwXCdy1LtD2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_words = [\"colour\", \"red\", \"green\", \"blue\", \"yellow\"]\n",
        "words_to_measure = [\"greenhouse\", \"sky\", \"grass\", \"purple\", \"scissors\",\n",
        "                    \"microphone\", \"president\"]\n",
        "\n",
        "results = []\n",
        "for word in words_to_measure:\n",
        "    avg_cosine_similarity, avg_category_cosine_similarity = # TODO\n",
        "    results.append((word, avg_cosine_similarity, avg_category_cosine_similarity))\n",
        "\n",
        "# Create a DataFrame to present the results in a table\n",
        "df_results = pd.DataFrame(results, columns=[\"Word\", \"Avg Cosine Similarity\", \"Cosine Similarity with Avg Embedding\"])\n",
        "print(df_results)"
      ],
      "metadata": {
        "id": "ZYMC0BtltHFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.1 Do the results for each method make sense? Why or why not? What is the apparent difference between method 1 and 2?\n",
        "\n",
        "Answer: TODO"
      ],
      "metadata": {
        "id": "KFc9BntQubV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training A Word Embedding Using the Skip-Gram Method on a Small Corpus\n",
        "\n",
        "So far in this notebook we've used the pre-trained GloVe embeddings. The lecture this morning described the Skip Gram method of training word embeddings. In this section you are going to review code to use that method to train a very small embedding, for a very small vocabulary on a very small corpus of text. The goal is to gain some insight into the general notion of how embeddings are produced. The corpus you are going to use is in the file SmallSimpleCorpus.txt, and was also shown in the lecture.\n",
        "\n",
        "NOTE: First we need to upload the file SmallSimpleCorpus.txt to the Colab environment. Download the data files from lab_5_1 folder on the github page and then navigate to the folder icon on the left hand side of this page and click the \"upload to session storage\" button to upload the data files to the colab session."
      ],
      "metadata": {
        "id": "moAuiKpZazXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy"
      ],
      "metadata": {
        "id": "whMJrpncbpAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "vPGnmHKPnvEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, read the file SmallSimpleCorpus.txt so that you see what the sequence of sentences is. Recalling the notion “you shall know a word by the company it keeps,” find three pairs of words that this corpora implies have similar or related meanings. For example, ‘he’ and ‘she’ are one such example – which you cannot use in your answer!\n",
        "\n",
        "Answer: TODO"
      ],
      "metadata": {
        "id": "tzqJDqfqn1uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file to colab and read in the corpus\n",
        "with open('./SmallSimpleCorpus.txt', 'r') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "# Preprocess the text\n",
        "def prepare_texts(corpus):\n",
        "    doc = nlp(corpus)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "91-JsD1_oNex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the prepare_texts function in the code given to you fulfills several key functions in text processing, a little bit simplified for this simple corpus. Rather than full tokenization it only lemmatizes the corpus,\n",
        "which means converting words to their root - for example the word “holds” becomes “hold”, whereas the word  “hold” itself stays the same.\n",
        "The prepare_texts function performs lemmatization using the [spaCy](https://spacy.io/models/en) library.\n",
        "Review the code of prepare_texts to make sure you understand what it is doing. Review the code that reads the corpus SmallSimpleCorpus.txt, and run the prepare_texts on it to return the text (lemmas) that will be used next.\n"
      ],
      "metadata": {
        "id": "zXaIvfftowCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lematize the corpus and create the vocabulary\n",
        "lemmas = #TODO\n",
        "vocab = set(lemmas)\n",
        "v2i = {v: i for i, v in enumerate(vocab)} # dictionary to lookup word to index\n",
        "i2v = {i: v for v, i in v2i.items()} # dictionary to lookup index to word"
      ],
      "metadata": {
        "id": "_kljxBXuoPxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the vocabulary size is 11. \\\\\n",
        "Which is the most frequent word in the corpus, and the least frequent word? \\\\\n",
        "What purpose do the v2i and i2v dictionaries serve?"
      ],
      "metadata": {
        "id": "jP-g4libpqa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n"
      ],
      "metadata": {
        "id": "N1-zfe8TpvQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `tokenize_and_preprocess_text` takes the lemmatized small corpus as input, along with `v2i` (which serves as a simple, lemma-based tokenizer) and a window size window. Its output should be the Skip Gram training dataset for this corpus: pairs of words in the corpus that “belong” together, in the Skip Gram sense.\n",
        "That is, for every word in the corpus a set of training examples are generated with that word serving as the (target) input to the predictor,\n",
        "and all the words that fit within a window of size window surrounding the word would be predicted to be in the “context” of the given word.\n",
        "The words are expressed as tokens (numbers).\n",
        " Add a little code so that you can see the dataset that is produced."
      ],
      "metadata": {
        "id": "66Ois3z7qS9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess the text\n",
        "def tokenize_and_preprocess_text(lemmas, v2i, window=3):\n",
        "    data = []\n",
        "    for i in range(len(lemmas)):\n",
        "        target = v2i[lemmas[i]]\n",
        "        context = []\n",
        "        for j in range(i - window // 2, i + window // 2 + 1):\n",
        "            if j != i and j >= 0 and j < len(lemmas):\n",
        "                context.append(v2i[lemmas[j]])\n",
        "        for c in context:\n",
        "            data.append((target, c))\n",
        "    return data"
      ],
      "metadata": {
        "id": "52WgB7prqPIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Skip gram dataset with window size of 5\n",
        "window_size = # TODO\n",
        "data = tokenize_and_preprocess_text(lemmas, v2i, window_size)\n"
      ],
      "metadata": {
        "id": "KOZ_-mqYqQNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review the code in Word2vecModel. Part of this model ultimately provides the trained embeddings/vectors, and you can see these are defined and initialized to random numbers in the line `self.embedding = torch.nn.Parameter(torch.rand(\n",
        "            vocab_size, embedding_size))`"
      ],
      "metadata": {
        "id": "cyQIHdXssbI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Word2Vec model\n",
        "class Word2VecModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super(Word2VecModel, self).__init__()\n",
        "        self.embedding = torch.nn.Parameter(torch.rand(\n",
        "            vocab_size, embedding_size))\n",
        "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding[x]\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tUti3LGZsZLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the vocab size\n",
        "vocab_size = # TODO\n",
        "embedding_size = 2 # Size of the embedding vector\n",
        "# Initialize the Word2Vec model\n",
        "model = # TODO"
      ],
      "metadata": {
        "id": "1SrHFoHns34Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review the code for training the model. It uses Cross Entropy loss function described in the lecture, a batch size of 4, a window size of 5, and 50 Epochs of training. It uses the Adam optimizer, and a learning rate of 0.001."
      ],
      "metadata": {
        "id": "Su1mTcsFtWKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "def train_word2vec(model, data, epochs=50, batch_size=4, learning_rate=0.001):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        np.random.shuffle(data)\n",
        "        losses = []\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i+batch_size]\n",
        "            inputs, labels = zip(*batch)\n",
        "            inputs = torch.tensor(inputs, dtype=torch.long)\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {np.mean(losses):.4f}')\n",
        "\n",
        "train_word2vec(model, data)"
      ],
      "metadata": {
        "id": "tXQOkAgkgCs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below that displays each of the embeddings in a 2-dimensional plot using Matplotlib.\n"
      ],
      "metadata": {
        "id": "BcJMpAzrvC--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the embeddings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_embeddings(model, i2v):\n",
        "    embeddings = model.embedding.data.numpy()\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, word in i2v.items():\n",
        "        x, y = embeddings[i]\n",
        "        plt.scatter(x, y)\n",
        "        plt.text(x + 0.02, y + 0.02, word, fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "plot_embeddings(model, i2v)"
      ],
      "metadata": {
        "id": "HjUiXq6chAnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the results make sense, and confirm your choices from part 1 of this Section?\n",
        "Answer: TODO\n",
        "\n",
        "What would happen when the window size is too large?\n",
        "Answer: TODO\n",
        "\n",
        "At what value would window become too large for this corpus?\n",
        "Answer: TODO"
      ],
      "metadata": {
        "id": "CRYB0G7tvSxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training A Single-Neuron Classifier to Determine if a Sentence is Objective or Subjective\n",
        "\n",
        "The purpose of this exercise is to review the code for training a simple network (just a single neuron) to determine if a sentence is objective or subjective."
      ],
      "metadata": {
        "id": "73X6qkEznL_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "0qh5A_xW2HuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from torchtext.vocab import GloVe\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "nkW2BxazxnED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file data.tsv in colab and Load the data\n",
        "df = pd.read_csv('data.tsv', sep='\\t', header=None, names=['sentence', 'label'])\n",
        "df = df.loc[1:]"
      ],
      "metadata": {
        "id": "p9fZDTPnwGZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a quick look at the file data.tsv to see which sentences were labelled subjective (1) and which objective (0). (The 1’s are in the first half of the file)"
      ],
      "metadata": {
        "id": "GuxuwOWDwmsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read through each of the code blocks, getting a rough sense of what is going on by reading the comments.\n",
        "You see code functions that split the dataset, in the tab-separated file data.tsv into training, validation and test sets.\n",
        "Perhaps look closest at the code block call “Classifier model” class where you can see the torch.nn.Linear class being used to instantiate a single neuron with embedding_size inputs and just 1 output."
      ],
      "metadata": {
        "id": "lY1VMmgL5EPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SpaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load GloVe embeddings using torchtext\n",
        "glove = GloVe(name='6B', dim=50)\n",
        "embedding_size = 50  # Size of GloVe embeddings\n",
        "\n",
        "# Function to convert a sentence to an embedding by averaging word embeddings\n",
        "def sentence_to_embedding(sentence, glove):\n",
        "    tokens = [token.text for token in nlp(sentence) if token.is_alpha]\n",
        "    embeddings = [glove[token] for token in tokens if token in glove.stoi]\n",
        "    if embeddings:\n",
        "        return torch.mean(torch.stack(embeddings), dim=0)\n",
        "    else:\n",
        "        return torch.zeros(embedding_size)\n",
        "\n",
        "# Convert all sentences to embeddings at once\n",
        "def convert_sentences_to_embeddings(sentences, glove):\n",
        "    embeddings = [sentence_to_embedding(sentence, glove) for sentence in sentences]\n",
        "    return torch.stack(embeddings)\n",
        "\n",
        "# Splitting the data into train, validation, and test sets\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
        "    df['sentence'], df['label'], test_size=0.2, random_state=42)\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
        "    train_sentences, train_labels, test_size=0.125, random_state=42)  # 0.125 * 0.8 = 0.1\n",
        "\n",
        "train_embeddings = convert_sentences_to_embeddings(train_sentences, glove)\n",
        "val_embeddings = convert_sentences_to_embeddings(val_sentences, glove)\n",
        "test_embeddings = convert_sentences_to_embeddings(test_sentences, glove)"
      ],
      "metadata": {
        "id": "2y6wogi63BKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding = self.embeddings[idx]\n",
        "        label = self.labels[idx]\n",
        "        return embedding, torch.tensor(float(label), dtype=torch.float32)\n",
        "\n",
        "# Convert to dataset\n",
        "train_dataset = TextDataset(train_embeddings, train_labels.tolist())\n",
        "val_dataset = TextDataset(val_embeddings, val_labels.tolist())\n",
        "test_dataset = TextDataset(test_embeddings, test_labels.tolist())\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "1OToKZZs9vpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classifier model\n",
        "class ClassifierModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(ClassifierModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return torch.sigmoid(x)\n"
      ],
      "metadata": {
        "id": "5976y14B6BYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the model\n",
        "model = ClassifierModel(embedding_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        batch_train_losses = []\n",
        "        for embeddings, labels in train_loader:\n",
        "            labels = labels.view(-1, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(embeddings)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        batch_val_losses = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for embeddings, labels in val_loader:\n",
        "                labels = labels.view(-1, 1)\n",
        "\n",
        "                outputs = model(embeddings)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                batch_val_losses.append(loss.item())\n",
        "\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = np.mean(batch_train_losses)\n",
        "        val_loss = np.mean(batch_val_losses)\n",
        "        val_accuracy = correct / total\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "num_epochs = 20\n",
        "train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "# Plotting training and validation losses\n",
        "plt.figure(figsize=(5, 2.5))\n",
        "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
        "plt.plot(range(num_epochs), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting validation accuracy\n",
        "plt.figure(figsize=(5, 2.5))\n",
        "plt.plot(range(num_epochs), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Define the function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    test_losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for embeddings, labels in test_loader:\n",
        "            labels = labels.view(-1, 1)\n",
        "\n",
        "            outputs = model(embeddings)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_losses.append(loss.item())\n",
        "\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Loss: {np.mean(test_losses):.4f}, Accuracy: {accuracy:.4f}')\n",
        "\n",
        "print(\"\\n\")\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "8_-EzPeB2ZeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the remaining code, which installs the gradio package and sets up an easy-to-use inter- face with the trained model, that you can type in any sentence and have the model decide if it is subjective or objective. To use the interface, just type in the sentence and click the classify button. You may find that it is better at classifying longer sentences, as that is the nature of the dataset it was trained on."
      ],
      "metadata": {
        "id": "SV4zHHKCD0Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Gradio interface\n",
        "def classify_sentence(sentence):\n",
        "    embedding = sentence_to_embedding(sentence, glove)\n",
        "    embedding = embedding.unsqueeze(0)\n",
        "    output = model(embedding)\n",
        "    label = 'Subjective' if output > 0.5 else 'Objective'\n",
        "    return label\n",
        "\n",
        "gr.Interface(fn=classify_sentence, inputs=\"text\", outputs=\"text\").launch()"
      ],
      "metadata": {
        "id": "iXmfEDUY2hs2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}