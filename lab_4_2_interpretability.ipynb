{"cells":[{"cell_type":"markdown","metadata":{"id":"bQV7JO4uoIfk"},"source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/eldanc/mlbootcamp2024/blob/main/lab_4_2_interpretability.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","# UofT DSI-CARTE ML Bootcamp\n","#### June 13, 2024\n","#### Interpretability and Fairness - Lab 2, Day 4\n","#### Teaching team: Eldan Cohen, Nakul Upadhya, Hriday Chedda\n","##### Lab author: Nakul Upadhya modified by Eldan Cohen and Alex Olson\n","\n","As decision-making increasingly relies on artificial intelligence, the issue of fairness and equity in machine learning is rapidly becoming a larger concern. In this lab, we will introduce various metrics that help analyze the biases of our models, as well as techniques that can help mitigate these discrepancies.\n","\n","The main packages we will be using in this lab is `fairlearn` [1], `interpret`[5], and `shap` [4] along with all the other packages we have previously used.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONptgKWToJkj"},"outputs":[],"source":["## Install Packages\n","!pip install fairlearn\n","!pip install shap\n","!pip install xgboost\n","!pip install interpret\n","\n","## Import packages\n","import numpy as np\n","import pandas as pd\n","\n","# IGNORE THESE LINES. They just turn off some annoying messages\n","pd.options.mode.chained_assignment = None\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"]},{"cell_type":"markdown","metadata":{"id":"ocEDl1_giEte"},"source":["## Data\n","For this analysis, we will be working with the COMPAS dataset [2], a dataset used by the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system. This is a popular tool used in the United States to assess the risk of recidivism (likelihood of re-offending) for individuals involved in the criminal justice system. ***The system is currently actively employed by judges and parole boards to make decisions about bail, sentencing, and parole.***\n","\n","The COMPAS system has been criticized for exhibiting racial and gender biases. Many studies have suggested that the tool is more likely to label Black defendants as high-risk and White defendants as low-risk, even when controlling for other factors. This has raised concerns about fairness and potential discrimination in decision-making processes. For more information about COMPAS, I highly encourage reading the [Propublica article that broke the story](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).\n","\n","This highly controversial system and the data it uses perfectly demonstrates the need for interpretability in machine learning. Let's start by reading and preprocessing the data. Some of the code for preprocessing has been borrowed from [this notebook.](https://github.com/tsotne95/FairnessCompas/blob/master/Fairness_in_Classification_on_the_COMPAS_dataset.ipynb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLpKGkWijqG_","outputId":"4403d067-8366-4da2-d234-3806d147a15b"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Load data\n","df_original = pd.read_csv(\n","    \"https://raw.githubusercontent.com/tsotne95/FairnessCompas/master/compas-scores-two-years.csv\"\n",")\n","print(f\"Original Entries in dataset: {df_original.shape}\")\n","\n","# Preprocessing\n","df_clean = df_original.dropna(\n","    subset=[\"days_b_screening_arrest\"]\n",")  # dropping missing vals\n","df_clean = df_clean[\n","    (df_clean.days_b_screening_arrest <= 30)\n","    & (df_clean.days_b_screening_arrest >= -30)\n","    & (df_clean.is_recid != -1)\n","    & (df_clean.c_charge_degree != \"O\")\n","    & (df_clean.score_text != \"N/A\")\n","]\n","\n","# Reset index after removing rows\n","df_clean.reset_index(inplace=True, drop=True)\n","\n","# Keep relevant columns and rename\n","df_clean = df_clean[\n","    [\n","        \"sex\",\n","        \"age\",\n","        \"race\",\n","        \"juv_fel_count\",\n","        \"juv_misd_count\",\n","        \"juv_other_count\",\n","        \"priors_count\",\n","        \"two_year_recid\",\n","        \"c_charge_degree\",\n","    ]\n","]\n","df_clean.rename({\"c_charge_degree\": \"felony\"}, axis=1, inplace=True)\n","\n","# Label encoding for 'sex' and 'felony' columns\n","label_encoder = LabelEncoder()\n","df_clean[\"sex\"] = label_encoder.fit_transform(df_clean[\"sex\"])\n","df_clean[\"felony\"] = label_encoder.fit_transform(df_clean[\"felony\"])\n","\n","# Get dummies for 'race' column\n","df_dummies = pd.get_dummies(df_clean[\"race\"])\n","df_dummies.columns = [x for x in df_dummies.columns]\n","\n","# Drop 'race' column and concatenate the dummies dataframe\n","df_final = pd.concat([df_clean.drop(\"race\", axis=1), df_dummies], axis=1)\n","\n","# Remove NA values\n","df_final = df_final.dropna()\n","print(f\"Entries in dataset after preprocessing: {df_final.shape}\")\n","\n","# Define features and target\n","X = df_final.drop(\"two_year_recid\", axis=1)\n","y = df_final[\"two_year_recid\"]\n"]},{"cell_type":"markdown","metadata":{"id":"npFKbD2MY0FS"},"source":["Now that its been pre-processed, lets split it up into train and test datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAwRdDvIqExI","outputId":"e782547d-7661-4a7c-b15d-9c0ff0d3029d"},"outputs":[],"source":["\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.25, random_state=42\n",")\n","\n","# Print final dataset information\n","print(\n","    f\"There are {X_train.shape[0]} training data points and {X_test.shape[0]} testing points\"\n",")\n","print(f\"There are {X_train.shape[1]} features in the dataset\")"]},{"cell_type":"markdown","metadata":{"id":"KZxtRAjiAZJP"},"source":["## Model-Based Interpretability\n","\n","One way we can use machine learning is to explore potential systemic issues present in the data. One way to do this is to utilize models that are constrained so that their predictive mechanisms are innately understandable to humans.\n","\n","### Decision Tree\n","One approach that provides model-based interpretability is the Decision Tree. Once a decision tree is trained, we as humans can visualize and walk through its decision mechanisms.\n","\n","Lets try fitting a 3 layer decision tree on our model and print out the testing accuracy and what features are used.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7fcL9F4Wcmg","outputId":"67f3a9cb-731c-4d94-83d7-24078df7bc5e"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier, export_graphviz\n","from sklearn.metrics import accuracy_score\n","import graphviz # Package containing visualization tools\n","\n","tree_clf = DecisionTreeClassifier(max_depth = 3, random_state = 42)\n","\n","tree_clf.fit(X_train, y_train)\n","\n","accuracy = accuracy_score(y_test, tree_clf.predict(X_test)) * 100\n","print(f\"Test Accuracy: {accuracy :.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"id":"hVdJYAmFZBOw","outputId":"5938ee06-c334-479f-8ee7-1c7b03e20eab"},"outputs":[],"source":["from sklearn.tree import plot_tree\n","import matplotlib.pyplot as plt\n","\n","def plot_decision_tree(\n","    clf, feature_names, class_names=[\"False\", \"True\"], figsize=(20, 10), fontsize=8\n","):\n","    fig, ax = plt.subplots(figsize=figsize)\n","    plot_tree(\n","        clf,\n","        feature_names=feature_names,\n","        filled=True,\n","        ax=ax,\n","        impurity=False,\n","        class_names=class_names,\n","        fontsize=fontsize,\n","    )\n","    plt.show()\n","\n","\n","# usage\n","plot_decision_tree(tree_clf, X_test.columns)\n","\n","## This tree is shaded based on the number of data points that fall into that node\n","## and the label of those datapoints. Ex. A dark blue node means that most of\n","## training datapoints that fall into that node did commit a crime within\n","## two years of release. Dark orange is the opposite."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoWpeefQ6wBO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"MRnZYVUmeM0-","outputId":"2e89514d-d64e-4ace-9d50-34be75998b73"},"outputs":[],"source":["import plotly.express as px # Interactive Plotting Package\n","\n","tree_importances = pd.DataFrame()\n","tree_importances['feature'] = X_train.columns\n","tree_importances['tree_importance'] = tree_clf.feature_importances_\n","tree_importances.sort_values(by = 'tree_importance', inplace =True, ascending = False)\n","# Remove features without any importance\n","importances = tree_importances[tree_importances['tree_importance'] > 0]\n","px.bar(importances, x = 'feature', y = 'tree_importance')"]},{"cell_type":"markdown","metadata":{"id":"PuQdbZY_sTQH"},"source":["### Generalized Additive Models"]},{"cell_type":"markdown","metadata":{"id":"0YwvSkUZXy1w"},"source":["In many cases, the decision tree is not the ideal model and we may want to utilize alternatives. One potential alternative approach is the **Generalized Additive Model (GAM).**\n","\n","GAMs are inherently interpretable models that learn a shape function for each feature, and predictions are made by “querying” the shape function. Since these shape functions are independent, the impact of a feature on the prediction can be understood by visualizing these shape functions, making them highly explainable. Formally, a GAM's prediction ($g(x)$) is defined as:\n","$$\n","g(x) = \\sum_{i=1}^N f(x_i)\n","$$\n","\n","Lets train and visualize a GAM to see how this looks like. The GAM we will use today is the *Explainable Boosting Machine* (EBM) available in the `interpret` package. First lets fit the EBM:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAC6R1vp9MLJ","outputId":"864f035f-5ccd-48e9-9685-20b10f552b40"},"outputs":[],"source":["from interpret.glassbox import ExplainableBoostingClassifier\n","\n","gam_clf = ExplainableBoostingClassifier(interactions = 0, random_state = 42)\n","\n","gam_clf.fit(X_train, y_train)\n","\n","accuracy = accuracy_score(y_test, gam_clf.predict(X_test)) * 100\n","print(f\"Test Accuracy: {accuracy :.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"Vd5L43rsZZ87"},"source":["The `interpret` package provides the ability to visualize the feature importances of the GAM as well as the learned shape functions. You can navigate the different graphs using the dropdown."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":797},"id":"-lMQcT9dZZar","outputId":"8354fbe5-c27e-4947-f6c4-a4a1a22294a7"},"outputs":[],"source":["from interpret import show\n","\n","ebm_global = gam_clf.explain_global()\n","show(ebm_global)"]},{"cell_type":"markdown","metadata":{"id":"TFOUvB9Va8Qw"},"source":["\n","**Your Turn:**\n","*   What is most important features?\n","*   Do these importances line up with the decision tree explanations above?\n","*   For the most important features, can you describe the relationship between the feature and the model prediction?\n"]},{"cell_type":"markdown","metadata":{"id":"hkXRVLDoxsc0"},"source":["GAMs can also be extended to GA$^2$Ms, where we can model the pairwise interaction between various features. Formally, a GA2M's prediction ($g(x)$) is defined as:\n","$$\n","g(x) = \\sum_{i=1}^N f_i(x_i) + \\sum_{i=1}^{N} \\sum_{j>i}^N f_{i,j}(x_i,x_j)\n","$$\n","\n","Lets train a GA$^2$M with 5 possible interactions:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-ROanSsyVd_","outputId":"c6897013-253f-41c8-eb87-05fb71b91261"},"outputs":[],"source":["from interpret.glassbox import ExplainableBoostingClassifier\n","\n","ga2m_clf = ExplainableBoostingClassifier(interactions = 5, random_state = 42)\n","\n","ga2m_clf.fit(X_train, y_train)\n","\n","accuracy = accuracy_score(y_test, ga2m_clf.predict(X_test)) * 100\n","print(f\"Test Accuracy: {accuracy :.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"IvcIHERiy9pz"},"source":["The impact of a pairwise interaction can be visualized via a heatmap and can be displayed using the same code as before. In the feature importance plot, the interaction is displayed as `feature 1 & feature 2`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":797},"id":"ggIgtP3yyftM","outputId":"adaf083b-8054-4d75-8385-1954d45a35b1"},"outputs":[],"source":["from interpret import show\n","\n","ebm_global = ga2m_clf.explain_global()\n","show(ebm_global)"]},{"cell_type":"markdown","metadata":{"id":"1wTCWrjtX2rt"},"source":["## Post-Hoc Interpretability\n","\n","Not all models can provide intrinsic explanations. One example of this is SVM where it simply creates hyperplanes to seperate classes. So what do we do in this case? This is where *post-hoc interpretability* methods come into play. Post-hoc methods work y taking in a trained models, modifying the inputs, and examining how significantly the outputs changed.\n","\n","One such interpretability tool is SHAP (Shapely Additive Values). SHAP provides feature importance by using methods from game theory to estimate the contribution of each feature towards the final prediction.\n","\n","\n","SHAP can provide a sense of both local (explaining a single prediction) and global (explaining general prediction trends) interpretability.\n","\n","To start, we first need to train a model to explain. For this exercise, we will use SVM with a radial kernel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLi3Fte7DNFt","outputId":"32061567-8b88-42bc-c253-422e51702d4b"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","svc_clf = SVC(kernel = 'rbf', max_iter = 2000)\n","svc_clf.fit(X_train, y_train)\n","accuracy = accuracy_score(y_test, svc_clf.predict(X_test)) * 100\n","print(f\"Test Accuracy: {accuracy :.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"7FKmTwqiDNLu"},"source":["Now to start our explanation process. We start off by first creating a summary of our dataset (this is to make SHAP run faster) and creating our explainer object.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4OPT_6KEGV5"},"outputs":[],"source":["import shap\n","# rather than use the whole training set to estimate expected values, we summarize with\n","# a set of weighted kmeans, each weighted by the number of points they represent.\n","# this helps everything run faster\n","X_train_summary = shap.kmeans(X_train, 7)\n","\n","# Create the shap explainer by passing in our model's predict function and\n","# the summarized training set\n","ex = shap.KernelExplainer(svc_clf.predict, X_train_summary, feature_names = X_train.columns)\n","\n","# We are also only going to look at 100 points (to make it easier to visualize)\n","X_test_subset = X_test.sample(100, random_state = 42).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"wBe1Gvt-QeZ4"},"source":["\n","Let's first look into the local explainability provided by SHAP by examining what contributes to the predictions of the first datapoint in our testing subset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"iK7FNfz9H8uA","outputId":"ad3ff758-629d-4834-90e5-e9fc08f49945"},"outputs":[],"source":["shap.initjs()\n","first_datapoint = X_test_subset.iloc[0]\n","single_point_shap_values = ex.shap_values(first_datapoint)\n","shap.force_plot(ex.expected_value, single_point_shap_values, X_test_subset.iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"j0WphK7PLWmm"},"source":["In the plot above, feature values that increased the chance of the model predicting a readmission are in red and have arrows that point to the right (they provide a positive force) and feature values that detract from the probability of readmission are in blue and point to the left. The larger the arrow, the larger the contribution.\n","\n","**Your Turn**\n","\n","* What features seem to have the most negative impact to the end prediction of the data point you chose? What about the one with the most positive impact? *YOUR ANSWER HERE*\n","* Choose a different data point and see if you see any similarities in the features used and their impact towards the end prediction. *YOUR ANSWER HERE*\n"]},{"cell_type":"markdown","metadata":{"id":"MYfQcTgXMMI0"},"source":["#### Global Explanability\n","We can get a sense of global interpretability from SHAP by examining trends in the SHAP values across the variable values. To do this, we can generate a summary plot of the calculated values. NOTE: This may take a while....."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":734,"referenced_widgets":["d3f2cd00bef9419bbdc330e3b037ccd8","a0df8b547e8744638039e9c4260eabcc","a858a39b23f74eb3a150ed83ab3be31d","8de5e10516894dc6b1ff1c7ff58c1975","b4a03dcabc1a4d33b3409f339de86b5b","80b8c84976a64308b472ef652e4c41f5","4df51698d13947e898401d67c93961a5","8a698084a5874a20ac3d29009e56d22a","481450aadbc04d23af3ed712132e51d5","e0f02a3db2114dfe9add2fdb09d42d19","cf44cb32a09e475e8d3cbd653fe83d65"]},"id":"hC06z6H9QwiB","outputId":"0e89922b-b808-41e2-8a20-6e29b2a0d3b4"},"outputs":[],"source":["shap.initjs()\n","shap_values = ex.shap_values(X_test_subset)\n","shap.summary_plot(shap_values, X_test_subset)"]},{"cell_type":"markdown","metadata":{"id":"9NvmzymoWiag"},"source":["The color of the point reflects the value of a given feature in a given data point. For example, a red point in `felony` means that the feature took a value of 1 (true) and a blue point means a value of 0 (false). The X-axis of this plot represents SHAP contribution (the estimated impact on the end model prediction). By examining the distribution of the feature values across the x-axis, we can find what features may"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"aXfrs9GJK5-f","outputId":"2f81a554-9f79-484a-a35a-23ab15dd7553"},"outputs":[],"source":["abs_shap_values = np.abs(shap_values)\n","shap_sums = np.sum(abs_shap_values, axis=0)\n","importances = pd.DataFrame()\n","importances['feature'] = X_train.columns\n","importances['importance'] = shap_sums\n","importances.sort_values(by = 'importance', inplace =True, ascending = False)\n","importances = importances[importances['importance'] > 0]\n","px.bar(importances, x = 'feature', y = 'importance')"]},{"cell_type":"markdown","metadata":{"id":"6vvC9-mfM1iM"},"source":["**Your Turn**\n","\n","* What features seem important to the SVM model according to SHAP? *YOUR ANSWER HERE*\n","* Do these SHAP importances align with the EBM and Decision Tree Importances? *YOUR ANSWER HERE\n","\n","Now consider all three models and the feature importances found.\n","* Are there any pattern that make sense to you? *YOUR ANSWER HERE*\n","* Are there any patterns that seem concerning from an equity perspective? *YOUR ANSWER HERE*"]},{"cell_type":"markdown","metadata":{"id":"mIsp4G29aZxB"},"source":["#### Notes about SHAP\n","SHAP is an incredibly powerful tool to understand what your model may be doing, ***however it is only an estimate***. The SHAP value calculations only examine your model's behavior and do not dive into the internals of the model, therefore these values should not be taken at face value. Additionally, as you may have noticed in the plots above, SHAP values do not reflect interacting effects between features, something that most models do in fact use. This extends to other post-hoc interpretability methods as well.\n","\n","As such, it is highly encouraged to use innately interpretable models whenever possible. For a more rigourous justification, please read Cynthia Rudin's paper on the subject [3] after the lab.\n","\n","Additionally, quoting the SHAP documentation [4]:\n","\n","\n","> Predictive machine learning models like XGBoost become even more powerful when paired with interpretability tools like SHAP. These tools identify the most informative relationships between the input features and the predicted outcome, which is useful for explaining what the model is doing, getting stakeholder buy-in, and diagnosing potential problems. It is tempting to take this analysis one step further and assume that interpretation tools can also identify what features decision makers should manipulate if they want to change outcomes in the future. However, in [this article](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html), we discuss how using predictive models to guide this kind of policy choice can often be misleading.\n","\n","> *Eleanor Dillon, Jacob LaRiviere, Scott Lundberg, Jonathan Roth, and Vasilis Syrgkanis from Microsoft.*\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"maL8tP4Ouh_I"},"source":["\n","## Fairness\n","Now that we have understood a bit of what are model is looking at and in general how it handles the features, lets examine how fair and equitable our models performance is. Ideally, a fair model should perform identically across different sensitive identities.\n","\n","Lets examine the fairness of our Decision Tree predictions\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGoOLPAZOCBZ"},"outputs":[],"source":["y_pred = tree_clf.predict(X_test) # Save the Test Predictions"]},{"cell_type":"markdown","metadata":{"id":"QpxsMDK3d8b_"},"source":["#### Measurement\n","The first step in ML fairness is to measure how fair the model is. For this, we will introduce two metrics.\n","\n","The first measure is *Demographic Parity*, or more accurately, the distance from demographic parity. Demographic parity is achieved when the probability of a certain prediction is not dependent on a point being in a sensitive group. This metric takes a range between 0 and 1 where 0 means we have achieved perfect demographic parity with respect to that feature.\n","\n","In general, a parity of under 20% is acceptable in many countries like the United States [1] to avoid legal problems, **however this should not be the goal as \"legally acceptable\" is not equal to \"fair\"**, especially in a high-stakes application such as law (like the dataset we are working with). Read more about the 4/5ths fallacy in the [fairlearn documentation](https://fairlearn.org/v0.8/user_guide/assessment/common_fairness_metrics.html#the-four-fifths-rule-often-misapplied) [1] after the lab.\n","\n","**Your Turn**\n","* For this problem in particular, what do you think an acceptable parity difference would be? *YOUR ANSWER HERE*\n","* Run the cell below that evaluates the difference between African-American individuals and non-African-American individuals. Is this model fair with regards to this feature? *YOUR ANSWER HERE*\n","* Test your own sensitive feature by changing `feature_under_examination`.Try a few features to see if you can find one that exceeds the margin you decided. Were you able to find one? *YOUR ANSWER HERE*\n","\n","Tip: Get all features in our data used by running `X_test.columns`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0J_ur4lk3xS","outputId":"e75dbd69-0214-458f-d65c-7a44d13b2172"},"outputs":[],"source":["from fairlearn.metrics import demographic_parity_difference\n","feature_under_examination = 'African-American'\n","parity_difference = demographic_parity_difference(y_test,\n","                                    y_pred,\n","                                    sensitive_features=X_test[feature_under_examination])\n","parity_difference = np.round(parity_difference * 100, 3)\n","print(f\"The demographic parity difference for {feature_under_examination} is {parity_difference}%\")"]},{"cell_type":"markdown","metadata":{"id":"1SzhQV7WoNdW"},"source":["The next metric is Equalized Odds. If equalized odds are achieved, that means that the difference in true positive rates and true negative rates across classes is the same. Often times, we measure the maximum difference between these metrics across the classes. For example, if the TPR and TNR for men were 75% and 65% and the rates for women were 73% and 52%, we would report back 13%.\n","\n","Similar to demographic parity, we want this to be as low as possible. Additionally, this metric is also subject to the 4/5ths fallacy as well.\n","\n","**Your Turn**\n","* Between demographic parity and equalized odds, which is a harder criteria to achieve and why? *YOUR ANSWER HERE*\n","* For this problem in particular, what do you think an acceptable equalized odds difference would be? *YOUR ANSWER HERE*\n","* Run the cell below that evaluates the difference between African-American individuals and non-African-American individuals. Is this model fair with regards to this feature? *YOUR ANSWER HERE*\n","* Test your own sensitive feature by changing `feature_under_examination` in the cell below.Try a few features to see if you can find one that exceeds the margin you decided. Were you able to find one? *YOUR ANSWER HERE*\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmk1WHiRk4HT","outputId":"533241ef-6503-4ca0-8f15-36e0770c36c2"},"outputs":[],"source":["from fairlearn.metrics import equalized_odds_difference\n","feature_under_examination = 'African-American'\n","eo_difference = equalized_odds_difference(y_test,\n","                                    y_pred,\n","                                    sensitive_features=X_test[feature_under_examination])\n","eo_difference = np.round(eo_difference * 100, 3)\n","print(f\"The Equalized Odds difference for {feature_under_examination} is {eo_difference}%\")"]},{"cell_type":"markdown","metadata":{"id":"ofN6cU52ujVP"},"source":["#### Mitigation\n","One way of mitigating unfairness is by adding constraints on the differences we mentioned above. This is done through the ExponentiatedGradient reduction where we re-weight samples during the training process until our metric (equalized odds or demographic parity) is below a set threshold in our training set.\n","\n","\n","Lets refit the model you chose with a Equalized Odds constraint with regards to African-American individuals like before.\n","\n","**Your Turn**\n","* Change `maximum_difference` to be the difference you find acceptable\n","* Run the cell below and report back the equalized odds difference on the testing set. Did you achieve your fairness goal? If not, did the fairness improve? *YOUR ANSWER HERE*\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPG9b93W0XaB","outputId":"bdad4e75-07a2-4d4a-9062-34a4988d55b5"},"outputs":[],"source":["from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient\n","## Create the model\n","model = DecisionTreeClassifier(max_depth=4)\n","\n","maximum_difference = .50 # change this to your acceptable difference\n","\n","feature_under_examination = 'African-American'\n","\n","reduction = ExponentiatedGradient( # Initialize the reduction mechanism\n","    model,\n","    EqualizedOdds(difference_bound = maximum_difference),\n","    eps = 1e-3\n",")\n","## THIS WILL TAKE A LONG TIME\n","reduction.fit(X_train, y_train, sensitive_features = X_train[feature_under_examination]) # Run the reduction mechanism\n","\n","y_pred_reduced = reduction.predict(X_test) # Make a prediction with the fair model\n","\n","eo_intersect_dif = equalized_odds_difference(y_test,\n","                                    y_pred_reduced,\n","                                    sensitive_features=X_test[feature_under_examination])\n","eo_intersect_dif = np.round(eo_intersect_dif * 100, 3)\n","print(f\"The Equalized Odds difference for {feature_under_examination} is {eo_intersect_dif}%\")"]},{"cell_type":"markdown","metadata":{"id":"HorZkT-K4MOf"},"source":["## References\n","1. Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., … Walker, K. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI. Retrieved from Microsoft website: https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/\n","2. Larson, J., Mattu, S., Kirchner, L., Angwin,J. (2016). How We Analyzed the COMPAS Recidivism Algorithm. https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\n","3. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell 1, 206–215 (2019). https://doi.org/10.1038/s42256-019-0048-x\n","4. Lundberg, S. M., & Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 30 (pp. 4765–4774). Retrieved from http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\n","5. Nori, H., Jenkins, S., Koch, P., & Caruana, R. (2019). Interpretml: A unified framework for machine learning interpretability. arXiv preprint arXiv:1909.09223.\n","\n","\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"481450aadbc04d23af3ed712132e51d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4df51698d13947e898401d67c93961a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80b8c84976a64308b472ef652e4c41f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a698084a5874a20ac3d29009e56d22a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8de5e10516894dc6b1ff1c7ff58c1975":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0f02a3db2114dfe9add2fdb09d42d19","placeholder":"​","style":"IPY_MODEL_cf44cb32a09e475e8d3cbd653fe83d65","value":" 100/100 [00:22&lt;00:00,  9.18it/s]"}},"a0df8b547e8744638039e9c4260eabcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80b8c84976a64308b472ef652e4c41f5","placeholder":"​","style":"IPY_MODEL_4df51698d13947e898401d67c93961a5","value":"100%"}},"a858a39b23f74eb3a150ed83ab3be31d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a698084a5874a20ac3d29009e56d22a","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_481450aadbc04d23af3ed712132e51d5","value":100}},"b4a03dcabc1a4d33b3409f339de86b5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf44cb32a09e475e8d3cbd653fe83d65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3f2cd00bef9419bbdc330e3b037ccd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0df8b547e8744638039e9c4260eabcc","IPY_MODEL_a858a39b23f74eb3a150ed83ab3be31d","IPY_MODEL_8de5e10516894dc6b1ff1c7ff58c1975"],"layout":"IPY_MODEL_b4a03dcabc1a4d33b3409f339de86b5b"}},"e0f02a3db2114dfe9add2fdb09d42d19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
